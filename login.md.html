<meta charset="utf-8"><!-- -*- markdown -*- -->

**The Road to Less Trusted Code: Lowering the Barrier to In-process Sandboxing**

Firefox currently ships with a variety of third-party and in-house libraries
running sandboxed, in-process, using a new framework called RLBox. We explore how
RLBox uses the C++ type system to simplify retrofitting sandboxing in existing
code bases, and consider how better tooling and architecture support can enable
a future where library sandboxing is a standard part of how we secure applications.


# Introduction

Users expect featureful software, and features, it hardly needs
saying, come from code.  The more features, the more code to implement
them.  And the more code, the more bugs---the more _security_ bugs, in
particular.

The latest code rushed out before a marketing deadline, or old code that
hasn't been touched since the developer who wrote it retired, or a
specialized module you licensed: attackers will scour all of them for
bugs to use for exploiting your software and targeting your users.

The problem is especially acute with third-party open source
libraries.  You might care about one thing the library does, but you
ship the whole library, and bugs in any part of it can create
security problems in your product.  (Unless you fork the library to
remove the extraneous code, but who wants to maintain a fork forever?)
Worse, hackers who find a bug in a popular library can try to deploy
it against every product that embeds the library---including yours.

Computer scientists have been thinking about software insecurity for
fifty years, and they have come up with approaches to mitigate it.
Rewrite your program (or parts of it) in a safer language!  Refuse to
ship new features and keep your program small!  Formally verify the
correctness of your software!  "Privilege separate" your system by
re-architecting it into multiple mutually-distrusting processes!  It's
fair to say that none of these approaches has solved the problem.
Insecure software is all around.

We believe that there is a practical path to improving software
security.  You can take software modules, including third-party
libraries, and _sandbox_ them to constrain what they can do---with low
programmer effort, reasonable runtime overhead, and without wholesale
rewriting or re-architecting---without even creating new OS processes.
The sandboxed module will still have bugs, but those bugs will not (in
most cases; see below) create security vulnerabilities in the
enclosing program.

Consider an image decoding library like libpng.  With sandboxing, we
can restrict this library so it has access to the image it decodes and
the bitmap it produces, _and that's it_.  Or, consider a
spell-checking library like Hunspell.  With sandboxing, we can
restrict this library to just its dictionary and the text it checks.
The application benefits from the library's features, but doesn't
inherit its security flaws.

Over the past two years we have worked with a team at Mozilla to build
a tool, called RLBox, to support sandboxing, and to migrate Firefox to
a model where many third-party libraries run sandboxed.  This new
approach is now shipping in Firefox.  Our experience suggests that
once there is sufficient tooling support that engineers can easily
sandbox libraries, they become increasingly comfortable with and
excited by the opportunities this offers.  For example, while the
initial target of our sandboxing collaboration was a third-party
font-shaping library, Graphite, now Firefox developers and security
engineers are using RLBox to sandbox both third-party libraries and
legacy Mozilla code in domains including media decoding, spell
checking, and even speech synthesis.

We believe that the opportunities extend far beyond Firefox.  After
all, secure messaging apps (e.g., Signal, WhatsApp, and iMessage),
servers and runtimes (e.g., Apache and Node.js), and enterprise tools
(e.g., Zoom, Slack, and VS Code) also rely on third party libraries
for various tasks---from media rendering, to parsing network protocols
like HTTP, image processing (e.g., to blur faces), spell checking, and
automated text completion.  With RLBox, these systems' developers are
empowered to sandbox modules and limit the damage their bugs can
cause.

Recent advances in compilers and processor architectures have made
efficient in-process isolation increasingly practical.  As it turns
out, though, keeping a module from reading or writing memory outside
its data region isn't enough.  Our initial efforts in manually
sandboxing Firefox libraries are a case in point.  Firefox had been
written under the assumptions that the libraries were trustworthy.
Even when isolated, they could return data values that would cause the
(unsandboxed) Firefox code to take unsafe actions, a scenario that
security researchers describe as a "confused deputy" attack.  We tried
to add code to manually check return values for consistency, but
repeatedly found that we had missed cases and left open avenues for
attack.

That's where RLBox comes in.  Using the C++ type system, RLBox
automatically generates the boilerplate code required for sandbox
interaction, and identifies _all_ places where the programmer will
have to add data-checking code.  With RLBox, programmers have a
framework that makes it easy to sandbox libraries (1) _securely_,
ensuring the interface between the untrusted library and the
application code is correct, and (2) with _minimal engineering
effort_, so that the cost of migrating libraries and applications to
sandboxing is not prohibitive.

In the rest of this article we describe the experience that led to
RLBox, how RLBox works and how it leverages the C++ type system to
make sandboxing practical, and how our type-driven approach can be
used in other domains (e.g., trusted execution environments).  Then we
outline how this approach can translate to languages other than C/C++.
Finally, we end with a vision of what software development could look
like with broader first-class support for sandboxing.

Before closing, we should note that sandboxing is not a panacea.  Some
components must be _correct_, not just isolated, for the system as a
whole to be secure.  The JavaScript just-in-time compilers used by Web
browsers are a notorious example.  With RLBox, you can sandbox
everything else, and focus developer time on getting these few
critical modules right.

# The road to RLBox: library sandboxing in Firefox

Firefox, like other browsers, relies on dozens of third-party libraries to
decode audio, images, fonts, and other content. These libraries have been a
significant source of vulnerabilities in the browser (e.g., most of the
exploitable bugs found by recent work using symbolic execution were in
third-party libraries [[**BSE20**][BSE20]]).  With collaborators at Mozilla, we
sought to minimize the damage due to vulnerabilities in libraries (e.g., a
bug in libvorbis was used to compromise Firefox at Pwn2Own 2018) by
retrofitting Firefox to sandbox these libraries.

When we began this project roughly two years ago, we thought the hardest part
would be adapting Google's Native Client (NaCl), a software-based isolation
(SFI) toolkit, to sandbox libraries. (NaCl is designed for sandboxing programs,
not libraries.) This turned out to be the easy part. Since then, WebAssembly (Wasm)
toolkits---in particular the Lucet Wasm compiler---have made this even
easier (see [[**NGLSS19**][NGLSS19]]).

In fact, the hardest part was the _last mile_, retrofitting Firefox to account
for the now-untrusted libraries.  Firefox was written assuming libraries are
trusted.  To add sandboxing, we had to change its threat model to assume
sandboxed libraries are untrusted, and harden the browser-library interface.
Hardening this interface in-turn required sanitizing data and regulating control flow
between sandboxed libraries and the browser. Thus, ensuring that malicious
libraries could not break out of their sandbox.

Our first attempt at sandboxing libraries in Firefox involved manually
hardening the library-application interface---this did not go well.

**Security Challenges**
To see how things can go wrong, let's consider
updating the `fill_input_buffer` JPEG decoder function. Libjpeg calls this
function whenever it needs more bytes from Firefox. As seen on line 16, Firefox
also saves the unused input bytes held by libjpeg to an internal back buffer,
which it sends to libjpeg along with the new input bytes.

```c++
 1: void fill_input_buffer (j_decompress_ptr jd) {
 2:   struct jpeg_source_mgr* src = jd->src;
 3:   nsJPEGDecoder* decoder = jd->client_data;
 4:   ...
 5:   src->next_input_byte = new_buffer;
 6:   ...
 7:   if (/* buffer is too small */) {
 8:     JOCTET* buf = (JOCTET*) realloc(...);
 9:     if (!buf) {
10:       decoder->mInfo.err->msg_code = JERR_OUT_OF_MEMORY;
11:       ...
12:     }
13:     ...
14:   }
15:   ...
16:   memmove(decoder->mBackBuffer + decoder->mBackBufferLen,
17:       src->next_input_byte, src->bytes_in_buffer);
18:   ...
19: }
````

When sandboxing libjpeg, we need to make the following changes:

- Sanitize `jd`, otherwise the read of `jd->src` on line 2 becomes an
  arbitrary-read gadget.
- Sanitize `src`, otherwise the write to `src->next_input_byte` on line 5
  becomes an arbitrary-write gadget and the `memmove()` on line 16 becomes an
  arbitrary read.
- Sanitize `jd->client_data` to ensure it points to a valid Firefox
  `nsJPEGDecoder` object, otherwise invoking a virtual method on it will hijack
  control flow.
- Sanitize the nested pointer `mInfo.err` on line 10 prior to dereferencing.
- Sanitize the pointer `decoder->mBackBuffer + decoder->mBackBufferLen` used on
  the destination address to `memmove()` on line 16. **TODO: shr, this seems wrong**
- Swizzle pointers like `mInfo.err` and `decoder->mBackBuffer`, i.e., translate
  the pointers to account for the potentially different machine model of the
  sandbox. Both NaCl and Wasm have different pointer representations.
- Ensure that multiple threads can't invoke the callback on the same image,
  otherwise we have a data race that results in a use-after-free vulnerability.

If we miss any of these checks---and these are only a limited sample of the kind
of checks required [[**NDG+20**][NDG+20]]---an attacker could potentially bypass
our sandbox through a confused deputy attack.  Not only was adding
these checks to the hundreds of Firefox functions that use libjpeg
tedious, but we frequently found checks we had overlooked.

**Engineering effort**
The upfront engineering effort of modifying the browser this way was huge.
Beyond adding security checks, we also had to retrofit all library calls, adjust
data structures to account for machine model (ABI) differences between the
application and sandbox (a common issue with SFI toolchains), marshal data to
and from the sandbox, etc.  Only then could we run tests to ensure our
retrofitting didn't break the application.  Finally, since Firefox runs on many
platforms---including platforms not yet supported by SFI toolkits like NaCl and
Wasm---we had to do this alongside the existing code that uses the library
unsandboxed, using the C preprocessor to select between the old code and the
new code. The patches to do all this became so complicated and unwieldy that we
couldn't imagine anybody maintaining our code, so we abandoned this manual
approach, built RLBox, and started anew.

# The RLBox framework

RLBox is a C++ library designed to make it easier for developers to securely
retrofit library sandboxing in existing applications.  It does this by making
data and control flow at the application-sandbox boundary explicit---using
types--and by providing APIs to both mediate these flows and enforce security
checks across the trust boundary.

RLBox mediates data flow using _tainted types_---it uses type wrappers to
demarcate data originating from the sandbox, and ensure
that application code cannot use this data unsafely.  For example, while
application code can add two `tainted<int>`s (to produce another
`tainted<int>`), it cannot branch on such values or use them as indexes into an
array. Instead, the application must validate tainted values before it can use
them.

RLBox mediates control flow with explicit APIs for control transfers.  Calls
into the sandbox must use `sandbox_invoke(sbx_fn, args...)`.  Callbacks into
the application can only use functions registered with the
`sandbox_callback(app_fn)` API.  These APIs also impose a strict data flow
discipline by forcing all sandbox function return values, and callback
arguments, to be tainted.

As we show next, this tainted-type-driven approach addresses both the security
and engineering challenges we outline above.

## Using tainted types to eliminate confused deputy attacks

RLBox eliminates confused deputy attacks by turning unsafe control- and
data-flows into type errors and, where possible, by performing automatic
security checks.  Concretely, RLBox automatically sanitizes sandbox-supplied
(tainted) pointers to ensure they point to sandbox memory, swizzles pointers that
cross the trust boundary, and statically identifies locations where tainted
data must be validated before use.

Consider, for example, the JPEG decoder callback from before. RLBox type errors
would guide us to (1) mark values from the sandbox as tainted (e.g., the `jd`
argument and `src` variable on line 2 below) and (2) _copy and verify_
(otherwise tainted) values we need to use (e.g., `jd->client_data` on line 3
below).

```c++
 1: void fill_input_buffer (rlbox_sandbox& sandbox, tainted<j_decompress_ptr> jd) {
 2:   tainted<jpeg_source_mgr*> src = jd->src;
 3:   nsJPEGDecoder* decoder = jd->client_data.copy_and_verify(...);
 4:   ...
 5:   src->next_input_byte = new_buffer;
 6:   ...
 7:   if (/* buffer is too small */) {
 8:     JOCTET* buf = (JOCTET*) realloc(...);
 9:     if (!buf) {
10:       decoder->mInfo.err->msg_code = JERR_OUT_OF_MEMORY;
11:       ...
12:     }
13:     ...
14:   }
15:   ...
16:   size_t nr = src->bytes_in_buffer.copy_and_verify(...));
17:   memmove(decoder->mBackBuffer + decoder->mBackBufferLen,
18:       src->next_input_byte.copy_and_verify(...), nr);
19:   ...
20: }
````

On line 3, 16, and 18 we need to write validators (as C++ lambdas to the
`copy_and_verify` method).  As we describe in [[**NDG+20**][NDG+20]], validators
fall into one of two categories: preserving application invariants (e.g.,
memory safety) or enforcing library invariants.  On line 3, for example, we
ensure that `decoder` points to a valid `nsJPEGDecoder` object not used by a
concurrent thread, while on line 16 we ensure that copying `nr` bytes won't
read past the `mBackBuffer` bounds.

We must get validators right---a bug in a validator is often a security bug.
In practice, though, validators are rare and short: The six libraries we
sandboxed in [[**NDG+20**][NDG+20]] required 2-14 validators each, and these
validators averaged only 2-4 lines of code. Most importantly, by making these
validators explicit, RLBox makes code reviews easier: security engineers only
need to review these validators.

What's missing in this code snippet is almost as important: we don't write
validators on lines 2, 5, and 10, for example. Instead, RLBox uses runtime
checks to automatically swizzle and sanitize the `src`, `src->next_input_byte`,
and `decoder->mInfo.err` pointers to point to sandbox memory.

## Using tainted types to minimize engineering effort

Manually migrating an application to use library sandboxing is labor intensive, and
demands a great deal of specific knowledge about the isolation mechanism.
RLbox abstracts away many of these specifics, making migration relatively
simple and mechanical.

**Incremental migration**
While RLBox automates many tasks, we still need to change application code
to use RLBox.  In particular, we need to add a trust boundary at the
library interface by turning all control transfers (i.e., library function
calls and callbacks) into RLBox calls, and we need to write validators to
sanitize data from the library, as we saw above.  Making these changes all at
once is frustrating, error-prone---overlooking a single change might
suddenly result in crashes or more subtle malfunctions---and hard to debug.

RLBox addresses these challenges with _incremental migration_, allowing
developers to modify application code to use the RLBox API one line at a time.
A full migration involves multiple steps, and is explained further in our paper
[[**NDG+20**][NDG+20]]. However, the key idea is that RLBox provides _escape
hatches_ which let developers temporarily disable some checks while migrating
their application code.  Thus, at each step, the application can be compiled,
run, and tested.

RLBox provides two escape hatches:

1. The **`UNSAFE_unverified` API** allows developers to temporarily remove the
   tainted type wrapper (to, for example, run and test their code). As the
   application is ported, calls to `UNSAFE_unverified` APIs are removed or
   replaced with validator functions that correctly sanitize tainted data.

2. The **RLBox noop sandbox** provides a pass-through sandbox that redirects
   function calls back to the unsandboxed version of the library, while still
   wrapping data as if it were received from a sandboxed library. This allows
   developers to use the RLBox APIs and test data validation separately from
   the actual isolation mechanism.

Compile-time type errors guide the developer by pointing to the next required
code change---e.g., data that needs to be validated before use, or control
transfer code that needs to change to use the RLBox APIs. By the end of the
process, the application is still fully functional, all the escape
hatches have been removed, and the application-library interface has
fully migrated to using tainted types.

We found that incremental migration greatly simplified the code review process.
In Firefox, we could commit and get reviews for partial migrations to the RLBox
API, since the Firefox browser continued to build and run nightly builds as
before. Additionally, we could explicitly include security reviews when writing
the data validators for tainted data.

Beyond migration, we also found the noop sandbox to be useful for selectively
enabling library sandboxing in conditional builds. For example, while Firefox
on Linux and OS&nbsp;X uses Wasm for isolation, Wasm runtime support for
Windows is incomplete and thus Firefox uses the noop sandbox on Windows builds;
once Windows support is complete, a single line change will allow us to take
advantage of the sandbox. This is useful beyond Firefox too: developers of the
Tor Browser (a downstream project of Firefox for anonymous web browsing), are
interested in sandboxing more libraries than mainline Firefox, since Tor users
typically have a higher security-performance threshold. Using the noop sandbox
will allow Tor developers to contribute upstream changes to sandbox libraries
in mainline Firefox, using the noop sandbox to avoid noticable overhead.  Tor
can then selectively enable additional sandboxing (again) with a one line
change, rather than having to maintain a major fork.

**ABI translations**
Isolation mechanisms can have different machine models and ABIs from the rest
of the application.  For example, Wasm is a 32-bit machine whose linear address
space is orthogonal to the rest of the application address space, and uses
types with different sizes and different machine representations.  Handling
these differences manually is laborious and error-prone.

Consider line 10 from the previous `fill_input_buffer` example where RLBox
automatically transforms pointers, and accounts for the difference in the size
of `long` and `void*`:

```c++
// mInfo is an object of type jpeg_decompress_struct
decoder->mInfo.err->msg_code = JERR_OUT_OF_MEMORY;
```

If we port this manually, the resulting code would look as follows.

```c++
auto err_field = adjust_for_abi_get_minfo_field(decoder->minfo, "err");
auto err_field_swizzled = adjust_for_abi_convert_pointer(err_field);
auto msg_field = adjust_for_abi_get_err_field(*err_field_swizzled, "msg_code");
assert(in_sandbox_memory(msg_field)); // Ensure pointer is in sandbox memory
auto msg_field_swizzled = adjust_for_abi_convert_pointer(msg_field);
// Assign the value
*msg_field_swizzled = adjust_for_abi(JERR_OUT_OF_MEMORY);
```

In contrast, RLBox requires no changes other than marking `mInfo` as tainted:

```c++
// mInfo is an object of type tainted<jpeg_decompress_struct>
decoder->mInfo.err->msg_code = JERR_OUT_OF_MEMORY;
```

RLBox can abstract and automatically reconcile ABI differences since all
control and data flow goes through explicit APIs and explicit types.

## Using tainted types outside of library sandboxing

The security challenges we face when sandboxing libraries are not unique to
library sandboxing. Developers have to handle untrusted data and control flow
in many other domains---and our tainted-type approach can help. We give three
examples.

**TEE runtimes:** Applications running in trusted execution environments
(TEEs), like Intel's SGX and ARM's TrustZone, interface with untrusted code by
design---TEEs even consider the OS untrusted. Getting this code right is hard.
And, unsurprisingly, TEE runtime libraries don't: Van Bulck et al.
[[**BOM+19**][BOM+19]], for example, found that most frameworks, across several
TEEs, were vulnerable to bugs RLBox addresses by construction.

**OS kernels:** Operating system kernels handle untrusted data from user
space.  Bugfinding tools---from MECA at the start of the century
[[**YKXE03**][YKXE03]] to Sys this year [[**BSE20**][BSE20]]---have found many
vulnerabilities in kernels due to unchecked (or improperly checked) user space
data (notably, pointers).  Frameworks like RLBox could automatically identify
where user space data needs to be checked and even perform certain checks
automatically (e.g., much like we ensure that sandbox pointers point to sandbox
memory, we can ensure that user space pointers point to user space memory).
Indeed, Johnson and Wagner's bugfinding tool [[**JW04**][JW04]] even used type
inference to find such kernel bugs.


**Browser IPC layers:** Modern browser architectures privilege separate
different parts of the browser into sandboxed processes. Almost all separate
the _renderer_ parts---the portion of the browser that handles untrusted user
content from HTML parsing, to JavaScript execution, to image decoding and
rendering---from the _chrome_ parts---the trusted portion of the browser that
can access the file system, network, etc.---and restrict communication to a
well-typed inter-process communication (IPC) layer. Like OS kernels, the
browser chrome must validate all values coming from untrusted renderer
processes; like kernels, browsers have been exploited because of unchecked (and
improperly checked) untrusted data. Here, again, tainted types can help---and
as a step in this direction, Mozilla started integrating tainted types into the
Firefox IPC layer, as part of the IPDL interface description language used to
generate boilerplate code for sending and receiving well-typed IPC messages
[[**R20**][R20]].


This list is by no means exhaustive; others have similarly observed that
tainting can be used to catch and prevent bugs when handling untrusted data
(e.g., see [[**XBS02**][XBS02]]).

# Beyond RLBox

We have thus far discussed RLBox in its current form---a framework that uses
the C++ type system, template metaprogramming, and  SFI toolkits like
Wasm to securely sandbox libraries typically written in C.  In the
future, we hope to see extensions to other languages, support for sandboxing
libraries written in arbitrary languages, and the adoption of processor
features that can further lower in-process sandboxing overheads.

## Beyond C++

We implemented RLBox in C++ because Firefox is predominantly written in C++.
To extend RLBox to other languages, we need to understand how to implement
RLBox's tainted type system.

Our C++ implementation uses templates to implement the generic `tainted<T>`
type, and takes advantage of function and operator overloading to make most of
the tainted type interface transparent.  For example, RLBox overloads pointer
dereferencing---the `->` and `*` operators---to allow dereferencing
`tainted<T*>` values safely, i.e., by automatically sanitizing the underlying
pointer to point to sandbox memory (line 10 in the `fill_input_buffer` example
above).  We also use template metaprogramming to enforce a custom type
discipline.

Many languages have features that are expressive enough to implement our
tainted type system directly or as part of the language toolchain (e.g.,
compiler plugins).

**Statically-typed languages**
RLBox is a natural fit for languages that already enforce type safety
statically.  Statically-typed languages typically offer some form of generics
or templates that can be used to implement tainted types.  Many also allow
function and operator overloading which, like C++, allows us to provide safe
operations on tainted types while preserving the original syntax of the
language.

Rust is a particularly compelling language.  First, Rust's raison d'Ãªtre is
safety---indeed, the language is used in many settings where assurance is a
paramount---and RLBox can compliment Rust's safety by, for example, making it
easy for Rust programmers to safely integrate C/C++ code into their projects
(which today is considered unsafe).  Second, Rust's macro system and support
for generics and operator overloading (via traits) allows tainted types to be
implemented directly in the language.  Finally, Rust's affine types can even
simplify certain RLBox validators (e.g., validators used to prevent
time-of-check-time-of-use and double fetch attacks).

**Dynamically-typed languages**
In dynamically-typed languages like JavaScript and Python, we can enforce
tainted types dynamically.  This, of course, makes the incremental porting loop
longer since type errors will only manifest at runtime. Luckily, many
industrial dynamically-typed languages have typed extensions to precisely
address this limitation (e.g., TypeScript and Flow extend JavaScript with
static type annotations).

**Compiler plugins and toolkits**
For languages not flexible enough to implement the RLBox tainted type system
statically, we envision implementing the type system as part of language
toolchains. For example, for C, we can implement RLBox as a Clang plugin (both to
enforce the type system and to generate runtime checks). Alternatively, we can
implement tainted types as part of interface description language (IDL)
compilers.  As mentioned above, for example, the Mozilla security team is
integrating tainted types into the Firefox IPDL inter-process communication
protocol IDL [[**R20**][R20]].

## Beyond software-based isolation

We designed RLBox to be make it easy for developers to plug-in different isolation
mechanisms. This makes it easy to migrate code (e.g., by using the noop
sandbox), as we describe above. It also allows developers to use different
isolation mechanisms that have different trade-offs.  For example, while in
production we use WebAssembly for isolation, in [[**NDG+20**][NDG+20]] we
evaluate two other isolation mechanisms: Wasm's predecessor NaCl, and
traditional process-based isolation.  These isolation mechanisms have different
trade-offs.  Process isolation is simple, but scales poorly---protection
boundary crossing costs become prohibitive as the number of sandboxes exceed
the number of available cores.  Wasm and NaCl, on the other hand, scale to a
large number of sandboxes and have cheap boundary crossings, but impose an
overhead on the sandboxed code.

At present, Wasm toolchains offer a practical and portable path to isolation.
But this software-based isolation approach will inevitably be slower than
running native code.

Hardware support for in-process isolation can offer solutions that are simple
and more performant. Today, for example, Intel's Memory Protection Key
features (MPK) incur roughly 1% overhead when used for in-process isolation
[[**VED+19**][VED+19]] (but this doesn't scale beyond 16 sandboxes). Looking
further out, the CHERI capability-based system will similarly make in-process
isolation---and memory safety more generally---cheap on ARM processors
[[**R19**][R19]]. By making it easy to use these features transparently
(e.g., for CHERI it can automatically adjust for ABI differences introduced
by capabilities), RLBox could lower the barrier to adopting new hardware
isolation features---and, we hope, this will encourage new hardware design for
in-process isolation.

## Bringing sandboxing to the developer ecosystem

While RLBox has been a boon for our work in Firefox, it's just a starting
point.  Our hope is that library sandboxing will become a first-class activity
in future development environments, and that RLBox's capabilities will
ultimately be subsumed by standard parts of tomorrow's languages, toolchains and
package managers. We believe in many cases such support could allow the use of
sandboxed libraries with a level of ease comparable to the use of unsandboxed
libraries today.

**FFIs and native code**
Many popular safe languages such as Python, Ruby, and JavaScript make
extensive use of native (typically C) code in their standard libraries and
package ecosystems via Foreign Function Interfaces (FFIs). Unfortunately,
bugs in native code can completely break all high-level safety guarantees.

Extending FFI interfaces and interface generation tools with first-class
support for sandboxing native code is very natural---both because the FFI
boundary is explicit and because developers are used to writing code that spans
trust boundaries.  Sandboxing unsafe code not only improves assurance, but can
also improve portability (e.g., by compiling native code to Wasm) and debugging
(native code has a long history of generating notoriously difficult bugs).

**Package managers**
In the ecology of package ecosystems there is constant competition between
package authors to provide the best package for a given task. Security is among
the ways that package authors have recently started differentiating their
package from others. We have seen this clearly in the Rust ecosystem, where the
presence (or absence) of unsafe code is one way that packages are compared.

Sandboxing is another way that package authors can provided differentiated
value, by integrating sandboxing support into their library.  This could look
like authors distributing their packages with most or all of the work required
to sandbox that package done upfront by the package author.  Developers could
then choose whether or not enable sandboxing with minimal additional fanfare.

To enable this, the package author could specify a system level sandboxing
policy (e.g., as a manifest file requesting access to parts of the file system
or network), and developers could then choose if and how to grant these
privileges when importing a package.  Much of the work of writing validators
for tainted types could also be mitigated by distributing validators as part of
a sandboxed library. We even envision an ecosystem of sandbox interface
declarations for existing packages, much like TypeScript type declarations for
JavaScript packages, which will allow to developers to pull sandboxed interfaces
much like they consume type interfaces today.


# Conclusions

Decades of attempts to detect and mitigate software vulnerabilities have
yielded lackluster results. Even browsers, some of the most heavily targeted
and scrutinized software, seem to provide an inexhaustible stream of
exploitable vulnerabilities.  In-process sandboxing can offer developers and
security engineers another choice---moving code (especially legacy and
third-party code) out of their trusted computing base by sandboxing it,
thus mitigating the impact of a compromise.

We developed RLBox to make in-process sandboxing practical. It is currently
being used to sandbox third-party and in-house libraries in Firefox, and we
hope that other C++ projects will choose to adopt it. Looking further out, we
hope to collaborate with developers of programming languages (and their
toolchains and standard libraries), package managers, and processor architects
to provide first-class support for in-process sandboxing. Small changes to
make in-process sandboxing first-class can result in huge benefits for
developers and security engineers.

# References

[**BOM+19**] J. Van Bulck, D. Oswald, E. Marin, A. Aldoseri, F. D. Garcia, and F. Piessens. "[A Tale of Two Worlds: Assessing the Vulnerability of Enclave Shielding Runtimes][BOM+19]". In _2019 ACM SIGSAC Conference on Computer and Communications Security (CCS '19)_. ACM, Nov. 2019.
[BOM+19]: https://people.cs.kuleuven.be/~jo.vanbulck/ccs19-tale.pdf

[**BSE20**] F. Brown, D. Stefan, and D. Engler. "[Sys: a Static/Symbolic Tool for Finding Good Bugs in Good (Browser) Code][BSE20]". In S. Capkun and F. Roesner, eds., _Proceedings of USENIX Security 2020_. USENIX, Aug. 2020.
[BSE20]: https://www.usenix.org/conference/usenixsecurity20/presentation/brown

[**JW04**] R. Johnson and D. Wagner. "[Finding User/Kernel Pointer Bugs With Type Inference][JW04]". In _Proceedings of USENIX Security 2004_. USENIX, Aug. 2004.
[JW04]: https://www.usenix.org/event/sec04/tech/full_papers/johnson/johnson_html/

[**NDG+20**] S. Narayan, C. Disselkoen, T. Garfinkel, N. Froyd, E. Rahm, S. Lerner, H. Shacham, and D. Stefan. "[Retrofitting Fine Grain Isolation in the Firefox Renderer][NDG+20]". In S. Capkun and F. Roesner, eds., _Proceedings of USENIX Security 2020_. USENIX, Aug. 2020.
[NDG+20]: https://www.usenix.org/conference/usenixsecurity20/presentation/narayan

[**NGLSS19**] S. Narayan, T. Garfinkel, S. Lerner, H. Shacham, and D. Stefan.  "[Gobi: WebAssembly as a Practical Path to Library Sandboxing][NGLSS19]".  arXiv:1912.02285, Dec. 2019.
[NGLSS19]: https://arxiv.org/abs/1912.02285

[**R19**] Richard Grisenthwaite. "[A Safer Digital Future, By Design][R19]". ARM Blueprint. Oct. 2019.
[R19]: https://www.arm.com/blogs/blueprint/digital-security-by-design

[**R20**] Tom Ritter. "[Support tainting data received from IPC][R20]". Mozilla Bug 1610005. Jan. 2020.
[R20]: https://bugzilla.mozilla.org/show_bug.cgi?id=1610005

[**VED+19**] A. Vahldiek-Oberwagner, E. Elnikety, N.O. Duarte, M. Sammler, P. Druschel, and D. Garg. "[ERIM: Secure, Efficient In-process Isolation with Protection Keys (MPK)][VED+19]".  In _Proceedings of USENIX Security 2019_. USENIX, Aug. 2019.
[VED+19]: https://www.usenix.org/conference/usenixsecurity19/presentation/vahldiek-oberwagner

[**XBS02**] W. Xu, S. Bhatkar, and R. Sekar. "[Taint-Enhanced Policy Enforcement: A Practical Approach to Defeat a Wide Range of Attacks][XBS02]" In _Proceedings of USENIX Security 2006_. USENIX, Aug. 2006.
[XBS02]: https://www.usenix.org/legacy/event/sec06/tech/full_papers/xu/xu_html/

[**YKXE03**] J. Yang, T. Kremenek, Y. Xie, and D. Engler.  "[MECA: an extensible, expressive system and language for statically checking security properties][YKXE03]".  In _Proceedings of the ACM conference on Computer and Communications Security_. ACM, Oct. 2003.
[YKXE03]: https://web.stanford.edu/~engler/ccs03-meca.pdf


# Authors

![](tal.jpg#biopic width=100) _Tal Garfinkel_ is an independant researcher and
consultant whose work focuses on the intersection of systems and security. He
received his PhD from Stanford University in 2010, and is a co-founder of the
Usenix Workshop on Offensive Technology.
talg@cs.stanford.edu

<br />

![](shravan.jpg#biopic width=100)_Shravan Narayan_ is a 5th-year PhD student
at UC San Diego working with Deian Stefan. His research focuses on in-process
sandboxing, WebAssembly, browser security and verified programming. He is the
maintainer of the RLBox sandboxing framework.
@srn002@cs.ucsd.edu

<br />

![](craig.jpg#biopic width=100) _Craig Disselkoen_ is a 5th-year PhD student
at UC San Diego under Deian Stefan and Dean Tullsen. His research focuses on
securing software through automatic vulnerability finding, program
transformations, and secure runtimes. He is the author of the Haybale
symbolic execution engine, written in Rust.
cdisselk@cs.ucsd.edu

<br />

![](hovav.jpg#biopic width=100) _Hovav Shacham_ is a professor of
computer science at the University of Texas at Austin.  His research
interests are in applied cryptography, systems security,
privacy-enhancing technologies, and technology policy.  His work has
been recognized with three "test-of-time" awards, including one at ACM
CCS 2017 for his 2007 paper that introduced return-oriented
programming.
hovav@cs.utexas.edu

<br />

![](deian.jpg#biopic width=100) _Deian Stefan_ is an Assistant Professor of CSE at UC San Diego, where he
co-leads the Security and Programming Systems groups.  He received his PhD from
Stanford University in 2016.  Deian was a cofounder of Intrinsic, a web
security start-up (acquired by VMWare).  His current research lies at the
intersection of secure systems, programming languages, and verification.
deian@cs.ucsd.edu


<style class="fallback">body{visibility:hidden;white-space:pre;font-family:monospace}</style>
<style>
img[src*="#biopic"] {
  float:left;
  margin-right: 1em;
  border: 1px solid black;
}
</style>
<script src="markdeep.min.js"></script>
<script>
  window.alreadyProcessedMarkdeep || (document.body.style.visibility="visible");
  markdeepOptions= {tocStyle: 'none', sortScheduleLists: false };
</script>
